main_6epsilons.py --dataset=CIFAR100 --model=resnet50 --pruning=True --tensorboard=True --log-interval=100 --pruning_config=./configs/CIFAR100_100_100.json --constraint=RobustnessDandR(eps1=7.8, eps2=2.9) --print-after-epoch=0 --dl2-weight=0.1 --delay=0 --epochs=100 --adv-after-epoch=0 --batch-size=16 --load_model=./CIFAR100/models/cifar100_resnet50_best.weights --name=CIFAR100_resnet50
pruning_engine.load_mask(): did not find mask file, will load nothing
=> loading checkpoint './CIFAR100/models/cifar100_resnet50_best.weights'
conv1.weight torch.Size([64, 3, 3, 3])
bn1.weight torch.Size([64])
bn1.bias torch.Size([64])
bn1.running_mean torch.Size([64])
bn1.running_var torch.Size([64])
bn1.num_batches_tracked torch.Size([])
layer1.0.conv1.weight torch.Size([64, 64, 1, 1])
layer1.0.bn1.weight torch.Size([64])
layer1.0.bn1.bias torch.Size([64])
layer1.0.bn1.running_mean torch.Size([64])
layer1.0.bn1.running_var torch.Size([64])
*********
conv1.weight torch.Size([64, 3, 3, 3])
bn1.weight torch.Size([64])
bn1.bias torch.Size([64])
bn1.running_mean torch.Size([64])
bn1.running_var torch.Size([64])
bn1.num_batches_tracked torch.Size([])
layer1.0.conv1.weight torch.Size([64, 64, 1, 1])
layer1.0.bn1.weight torch.Size([64])
layer1.0.bn1.bias torch.Size([64])
layer1.0.bn1.running_mean torch.Size([64])
layer1.0.bn1.running_var torch.Size([64])
=> loaded checkpoint './CIFAR100/models/cifar100_resnet50_best.weights' (epoch -1)
epoch, neuron_units, loss, dl2_loss, top1, top5, dl2_p_acc, dl2_constraint_acc
1, 7552, 10.2962, 0.9508, 0.000, 6.250, 0.010, 0.000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 100, 581.1143555641174 
1, 7452, 148.4880, 195.2455, 6.250, 6.250, 0.010, 0.140, 1.0000, 1.0000, 0.8333, 1.0000, 1.0000, 1.0000, 200, 1289.7793834209442 
skipping pruning 135.4891273164749 (135.489127) 100.0
1, 7452, 8.4290, 21.7531, 0.000, 0.000, 0.011, 0.511, 0.5333, 0.5000, 0.5000, 0.5556, 0.8182, 0.6667, 300, 1997.163803100586 
1, 7352, 7.7755, 0.3451, 0.000, 6.250, 0.012, 0.000, 0.6000, 1.0000, 0.4286, 1.0000, 0.8333, 0.7500, 400, 2706.511577606201 
1, 7252, 4.9914, 149.0252, 0.000, 12.500, 0.009, 0.415, 0.4000, 0.1429, 0.0000, 0.2000, 0.1429, 0.0000, 500, 3416.884173631668 
skipping pruning 118.03176359176636 (118.031764) 100.0
1, 7252, 4.7268, 0.6688, 0.000, 12.500, 0.008, 0.187, 0.5556, 0.2000, 0.7143, 1.0000, 0.6000, 0.6000, 600, 4119.476662158966 
1, 7152, 6.4808, 1.5775, 0.000, 6.250, 0.008, 0.621, 0.4000, 0.1250, 0.2000, 0.2000, 0.3333, 0.2500, 700, 4832.3290474414825 
1, 7052, 6.3398, 0.5126, 0.000, 18.750, 0.009, 0.552, 0.2857, 0.1667, 0.4000, 0.0000, 0.3333, 0.0000, 800, 5543.032420158386 
1, 6952, 7.1782, 2.7338, 0.000, 6.250, 0.016, 0.381, 0.6471, 0.3889, 0.4167, 0.4000, 0.4286, 0.6364, 900, 6271.237898349762 
1, 6852, 8.5980, 0.4835, 0.000, 0.000, 0.014, 0.550, 0.7500, 0.7778, 0.5385, 0.5556, 0.6364, 0.3000, 1000, 6979.915931940079 
1, 6752, 7.7250, 1.7332, 0.000, 6.250, 0.017, 0.523, 0.6000, 0.2500, 0.4444, 0.4000, 0.7143, 0.2143, 1100, 7369.596665859222 
1, 6652, 5.7572, 1.5373, 0.000, 6.250, 0.012, 0.775, 0.2500, 0.2857, 0.2000, 0.2222, 0.5000, 0.1818, 1200, 7759.514856815338 
1, 6552, 6.2340, 2.4865, 0.000, 0.000, 0.013, 0.658, 0.0909, 0.3000, 0.4545, 0.5000, 0.2222, 0.2727, 1300, 8151.782682657242 
1, 6452, 6.0297, 338.3141, 0.000, 6.250, 0.015, 0.392, 0.8571, 0.8333, 0.8750, 0.7000, 0.6667, 0.4444, 1400, 8543.112858057022 
1, 6352, 5.4949, 0.9279, 0.000, 18.750, 0.018, 0.700, 0.2857, 0.5789, 0.4375, 0.4444, 0.1429, 0.2308, 1500, 8941.2175822258 
1, 6252, 5.9931, 1.8277, 0.000, 6.250, 0.018, 0.759, 0.4545, 0.1111, 0.3750, 0.0714, 0.0714, 0.1000, 1600, 9337.309980392456 
1, 6152, 4.9932, 0.7329, 6.250, 12.500, 0.019, 0.595, 0.1429, 0.2857, 0.3000, 0.2500, 0.2857, 0.2727, 1700, 9734.042140960693 
1, 6052, 4.8887, 2.1552, 18.750, 25.000, 0.019, 0.698, 0.3636, 0.0000, 0.4615, 0.2500, 0.1111, 0.0769, 1800, 10130.846113681793 
1, 5952, 5.3916, 1.9018, 0.000, 6.250, 0.021, 0.708, 0.4091, 0.4667, 0.1667, 0.2000, 0.3529, 0.2143, 1900, 10535.00363278389 
1, 5852, 12.2305, 0.6652, 6.250, 6.250, 0.017, 0.657, 0.5455, 0.3571, 0.4286, 0.3333, 0.1333, 0.4286, 2000, 10934.743394851685 
1, 5752, 5.7030, 0.5783, 0.000, 0.000, 0.014, 0.646, 0.5000, 0.3750, 0.1818, 0.3750, 0.3636, 0.3333, 2100, 11333.435911417007 
1, 5652, 10.1138, 0.5351, 0.000, 12.500, 0.019, 0.629, 0.5000, 0.5000, 0.4444, 0.3571, 0.5000, 0.3529, 2200, 11735.188102006912 
1, 5552, 9.6737, 0.7882, 0.000, 6.250, 0.014, 0.696, 0.5714, 0.4000, 0.5000, 0.7273, 0.0000, 0.4545, 2300, 12136.517977237701 
1, 5452, 14.8105, 0.7245, 0.000, 12.500, 0.017, 0.551, 0.0000, 0.1000, 0.1429, 0.2308, 0.3077, 0.3636, 2400, 12540.901480197906 
